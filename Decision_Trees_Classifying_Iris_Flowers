# Decision Tree on the classic Iris dataset, evaluates it, and visualizes the tree using scikit-learn

# Notes:
# max_depth curbs overfitting; try None to see a fully grown tree
# Switch criterion between "gini" and "entropy" to compare split metrics.
# For a publication-quality graphic, you can also export with export_graphviz and render via Graphviz.

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# 1) Load data
iris = load_iris()
X, y = iris.data, iris.target
feature_names = iris.feature_names
class_names = iris.target_names

# 2) Split (stratify to preserve class balance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# 3) Train decision tree
# Tip: limit depth to reduce overfitting (Iris is easy; depth=3 works well)
clf = DecisionTreeClassifier(
    criterion="gini",       # or "entropy"
    max_depth=3,            # try None to grow fully
    random_state=42
)
clf.fit(X_train, y_train)

# 4) Evaluate
y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification report:\n", classification_report(y_test, y_pred, target_names=class_names))

# 5) Visualize the tree (simple Matplotlib plot)
plt.figure(figsize=(10, 6))
plot_tree(
    clf,
    feature_names=feature_names,
    class_names=class_names,
    filled=True,            # color leaves by class
    rounded=True
)
plt.title("Decision Tree on Iris")
plt.show()

